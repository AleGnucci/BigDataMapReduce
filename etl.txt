//apri putty, usa isi-vclust4.csr.unibo.it , con username agnucci

//creazione cartella utente e altre
hdfs dfs -mkdir /user/agnucci
hdfs dfs -mkdir /user/agnucci/datasets
hdfs dfs -mkdir /user/agnucci/outputMR
hdfs dfs -mkdir /user/agnucci/outputSpark

//poi carica i files in una cartella sul cluster con winSCP

//poi vai in quella cartella con il comando cd, dentro alla shell di putty

//caricamento di files su hdfs
hadoop fs -put ./ /user/agnucci/datasets/

//cancellazione di vecchia versione del dataset unito:
hadoop fs -rmr /user/agnucci/datasets/youtubeDataset

//con spark2-shell (unione di files e conversione a parquet): guarda il file EtlJob nel progetto BigDataSpark