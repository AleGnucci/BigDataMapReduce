//apri putty, usa isi-vclust4.csr.unibo.it , con username agnucci

//creazione cartella utente e altre
hdfs dfs -mkdir /user/agnucci
hdfs dfs -mkdir /user/agnucci/datasets
hdfs dfs -mkdir /user/agnucci/outputMR
hdfs dfs -mkdir /user/agnucci/outputSpark

//poi carica i files in una cartella sul cluster con winSCP

//poi vai in quella cartella con il comando cd, dentro alla shell di putty

//caricamento di files su hdfs
hadoop fs -put ./ /user/agnucci/datasets/

//cancellazione di vecchia versione del dataset unito:
hadoop fs -rmr /user/agnucci/datasets/youtubeDataset

//con spark2-shell (unione di files e conversione a parquet):

val sqlContext= new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType, BooleanType};
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
val CAdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/CAvideos.csv")
val DEdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/DEvideos.csv")
val FRdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/FRvideos.csv")
val GBdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/GBvideos.csv")
val INdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/INvideos.csv")
val JPdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/JPvideos.csv")
val KRdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/KRvideos.csv")
val MXdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/MXvideos.csv")
val RUdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/RUvideos.csv")
val USdf = spark.read.option("header", "true").option("delimiter", ",").option("inferSchema", "true").csv("hdfs:/user/agnucci/datasets/youtube-new/USvideos.csv")

val bigDf1 = CAdf.unionAll(DEdf.unionAll(FRdf.unionAll(GBdf.unionAll(USdf))))
//val bigDf2 = INdf.unionAll(JPdf.unionAll(MXdf.unionAll(MXdf.unionAll(RUdf)))) //il problema Ã¨ JP con MX e anche KR con MX
/*val bigDf1Corrected = bigDf2.withColumn("comments_disabled", $"comments_disabled".cast(StringType))
    .withColumn("ratings_disabled", $"ratings_disabled".cast(StringType))
    .withColumn("video_error_or_removed", $"video_error_or_removed".cast(StringType))
    */
//val bigDf = bigDf2.unionAll(bigDf2)

val outputDf = bigDf1.coalesce(1);
outputDf.write.parquet("hdfs:/user/agnucci/datasets/youtubeDataset")