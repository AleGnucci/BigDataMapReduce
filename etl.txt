//apri putty, usa isi-vclust4.csr.unibo.it , con username agnucci

//creazione cartella utente e altre
hdfs dfs -mkdir /user/agnucci
hdfs dfs -mkdir /user/agnucci/datasets
hdfs dfs -mkdir /user/agnucci/outputMR
hdfs dfs -mkdir /user/agnucci/outputSpark

//poi carica i files in una cartella sul cluster con winSCP

//poi vai in quella cartella con il comando cd, dentro alla shell di putty

//caricamento di files su hdfs
hadoop fs -put ./ /user/agnucci/datasets/

//con spark2-shell (unione di files e conversione a parquet):

val sqlContext= new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
val CArdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/CAvideos.csv")
val DErdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/DEvideos.csv")
val FRrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/FRvideos.csv")
val GBrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/GBvideos.csv")
val INrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/INvideos.csv")
val JPrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/JPvideos.csv")
val KRrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/KRvideos.csv")
val MXrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/MXvideos.csv")
val RUrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/RUvideos.csv")
val USrdd = sc.textFile("hdfs:/user/agnucci/datasets/youtube-new/USvideos.csv")
val bigRdd = sc.union(Seq(CArdd, DErdd, FRrdd, GBrdd, INrdd, JPrdd, KRrdd, MXrdd, RUrdd, USrdd))
val schema = new StructType()
  .add(StructField("video_id", StringType, true))
  .add(StructField("trending_date", DoubleType, true))
  .add(StructField("title", DoubleType, true))
  .add(StructField("channel_title", DoubleType, true))
  .add(StructField("category_id", DoubleType, true))
  .add(StructField("publish_time", DoubleType, true))
  .add(StructField("tags", DoubleType, true))
  .add(StructField("views", DoubleType, true))
  .add(StructField("likes", DoubleType, true))
  .add(StructField("dislikes", DoubleType, true))
  .add(StructField("comment_count", DoubleType, true))
  .add(StructField("thumbnail_link", DoubleType, true))
  .add(StructField("comments_disabled", DoubleType, true))
  .add(StructField("ratings_disabled", DoubleType, true))
  .add(StructField("video_error_or_removed", DoubleType, true))
  .add(StructField("description", DoubleType, true))
val outputRdd = bigRdd.coalesce(1)
spark.createDataFrame(outputRdd, schema).write.parquet("hdfs:/user/agnucci/datasets/youtubeDataset")

video_id,trending_date,title,channel_title,category_id,publish_time,tags,views,likes,dislikes,comment_count,thumbnail_link,comments_disabled,ratings_disabled,video_error_or_removed,description